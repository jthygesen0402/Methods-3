---
title: "A3_MachineLearning"
author: "Jakob, Emilie, Gergana & Natasha"
date: "2022-11-02"
output: html_document
---

# The assignment

The Machine Learning assignment has 3 main parts: 
- First we create a skeptical and an informed simulation, based on the meta-analysis. 
- Second we build and test our machine learning pipeline on the simulated data. 
- Second we apply the pipeline to the empirical data.

The report for the exam, thus, consists of the answer to all the following prompts:
- Describe your machine learning pipeline. 
- Produce a diagram of it to guide the reader (e.g. see Rybner et al 2022 Vocal markers of autism: Assessing the generalizability of ML models). 
- Describe the different parts: data budgeting, data pre-processing, model choice and training, assessment of performance.
- Briefly justify and describe your use of simulated data, and results from the pipeline on them.
- Describe results from applying the ML pipeline to the empirical data and what can we learn from them.

Remember: plots are very very important to communicate your process and results.

```{r}
pacman::p_load(tidyverse,purrr, brms, cmdstanr, bayesplot, rstanarm, viridis, gridExtra, RColorBrewer, reshape2, tidymodels, yardstick, corrplot, DALEX, DALEXtra, randomForest, xgboost, kernlab, caret, cvms, groupdata2, DataExplorer, klaR, dplyr, mlbench)


set.seed(3)
```


```{r}
color_scheme_set(scheme = "teal")
```




# Part I - Simulating data

Use the meta-analysis reported in Parola et al (2020), *create a simulated dataset* with 100 matched pairs of schizophrenia and controls, each participant producing 10 repeated measures (10 trials with their speech recorded). for each of these "recordings" (data points) produce 10 acoustic measures: 6 from the meta-analysis, 4 with just random noise. Do the same for a baseline dataset including only 10 noise variables. 
Tip: see the slides for the code. 


```{r}
#Setting up all the parameters of the simulation
people <- 200
trials <- 10

EffectsMeans_inform <- c(0.25,-0.55,-0.75,-1.26,0.05,1.89,0,0,0,0)
EffectsSD_inform <- c(0.5, 0.29, 0.39,0.63,0.59,0.62,1,1,1,1)

EffectsMeans_noise <- rep(0,10)
EffectsSD_noise <- rep(1,10)

TrialSD <- 0.5
Error <- 0.2
```


```{r}
#Creating a function that can simulate the true underlying value for all variables for a participant

true_val <- function(means, SDs){
  true_df <- data.frame(Map(rnorm, n=1, mean=means, sd=SDs))
  names(true_df) <- c(paste0("v", seq(10)))
  true_df <- as_tibble(true_df)
}
```



## Simulation of *informed data*

```{r}
#Making a df of individual effect sizes across variables (drawn from "true" means)
sim_d <- data.frame( 
  ID = seq(people/2))

sim_d$values <- map(EffectsMeans_inform, true_val, SDs=EffectsSD_inform) #map effect_means on the sim_d as a nested df

sim_d <- unnest(sim_d, cols = values)#Now we have a df with one effect size per variable per participant
```


```{r}
 # Adding trails
trial <- expand.grid(ID=seq(people/2),trial=seq(1,10))

sim_d <- merge(sim_d,trial) 

sim_d <- sim_d %>% 
  rowwise%>% 
  mutate(across(starts_with("v"), ~rnorm(1, mean = rnorm(1,mean=.x, sd=TrialSD), sd = Error), .names="{.col}_obs"))

# For each trial, simulate a value that is drawn from the underlying mean with TrialSD as SD and simulate error on top of that

#Making matched pairs
pairs <- expand.grid(group = c("SZ", "HC"), ID=seq(people/2))

sim_d <- merge(sim_d, pairs)

```


```{r}
#Differentiating between SZ and HC 
matched_data <- sim_d %>% 
  rowwise %>%  
  mutate(across(ends_with("_obs"), ~if_else(group == "SZ", (.x/2), -(.x/2)), .names ="{.col}_matched")) %>%
  dplyr::select(ID, trial, group, ends_with("_matched"))

```



## Simulation of *noise data*
```{r}
sim_d_noise <- data.frame( 
  ID = seq(people/2))

sim_d_noise$values <- map(EffectsMeans_noise, true_val, SDs=EffectsSD_noise) #map effect_means on the true_df as a nested df

sim_d_noise <- unnest(sim_d_noise, cols = values) #Now we have a df with one effect size per variable per participant
```

```{r}
 # Adding trials
sim_d_noise <- merge(sim_d_noise,trial) 

sim_d_noise <- sim_d_noise %>%
  rowwise %>% 
  mutate(across(starts_with("v"), ~rnorm(1, mean = rnorm(1,mean=.x, sd=TrialSD), sd = Error), .names="{.col}_obs"))

#Making matched pairs
sim_d_noise <- merge(sim_d_noise, pairs)

```


```{r}
#Differentiating between SZ and HC 
matched_noise<- sim_d_noise %>% 
  rowwise %>%  
  mutate(across(ends_with("_obs"), ~if_else(group == "SZ", (.x/2), -(.x/2)), .names ="{.col}_matched")) %>%
  dplyr::select(ID, trial, group, ends_with("_matched"))

```


#Visualiziing the data :(
```{r}
#Making the df into long format 
long_df <- matched_data %>%
  pivot_longer(cols = ends_with("_matched"), names_to = "variable")

long_df %>% 
  ggplot(aes(x = value, colour=group)) +  
  geom_density() + 
  scale_color_brewer(palette = "Paired") + 
  facet_wrap(~variable) + 
  theme_grey() + 
  ggtitle("Effect sizes across variables")


#Making the df into long format 
long_df_noise <- matched_noise %>%
  pivot_longer(cols = ends_with("_matched"), names_to = "variable")

long_df_noise %>% 
  ggplot(aes(x = value, colour=group)) +  
  geom_density() + 
  scale_color_brewer(palette = "Accent") + 
  facet_wrap(~variable) + 
  theme_grey() + 
  ggtitle("Effect sizes across variables on noise data")
```

# Part II - ML pipeline on simulated data

On the two simulated datasets (separately) build a machine learning pipeline: 
*i)* create a data budget (e.g. balanced training and test sets) 

*ii)* pre-process the data (e.g. scaling the features)

*iii)* fit and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression)

*iv)* assess performance on the test set

*v)* discuss whether performance is as expected and feature importance is as expected.

Bonus question: replace the bayesian multilevel regression with a different algorithm, e.g. SVM or random forest (but really, anything you'd like to try).



##Informed data

##Data budgetting and preprocessing on the informed data
```{r}
#Splitting data 80/20
spl <- sample(seq(people/2), people/2*0.2)

train_inf <- matched_data %>% 
  subset(!(ID %in% spl)) %>% 
  mutate(ID=as.factor(ID), trial=as.factor(trial))

test_inf <- matched_data %>% 
  subset(ID %in% spl) %>% 
  mutate(ID=as.factor(ID), trial=as.factor(trial))


```


```{r}
#Centering the training data
recipe_inf <- train_inf %>% recipe(group ~.) %>% 
  update_role(ID, trials, new_role = "Second level") %>%
  update_role_requirements("Second level", bake=F) %>% 
  step_scale(all_numeric()) %>%
  step_center(all_numeric()) %>%
  prep(train=train_inf, retain = T)
  

#Using the recipe on the test data
train_inf <- juice(recipe_inf)
test_inf_s<- bake(recipe_inf, new_data = test_inf)

test_inf_s$ID <- test_inf$ID

```


#Choosing a classification algorithm - Logistic Regression

## Data budgetting and preprocessing on the *noise* data set

```{r}
#Splitting data 80/20
train_noise <- matched_noise %>%
  subset(!(ID %in% spl)) %>%
  mutate(ID=as.factor(ID), trial=as.factor(trial))

test_noise <- matched_noise %>% 
  subset(ID %in% spl) %>% 
  mutate(ID=as.factor(ID), trial=as.factor(trial))

```


```{r}
#Centering the training data
recipe_noise <- train_noise %>% recipe(group ~.) %>% 
  update_role(ID, trials, new_role = "Second level") %>%
  update_role_requirements("Second level", bake=F) %>% 
  step_scale(all_numeric()) %>%
  step_center(all_numeric()) %>%
  prep(train=train_noise, retain = T)
  

#Using the recipe on the test data
train_noise <- juice(recipe_noise)
test_noise_s <- bake(recipe_noise, new_data = test_noise)

test_noise_s$ID <- test_noise$ID

```



## Setting up the model

### Choosing a classification algorithm - Logistic Regression - Setting priors
```{r}
#Setting up the formula

f1 <- bf(group ~ 1 + v1_obs_matched+ v2_obs_matched + v3_obs_matched + v4_obs_matched + v5_obs_matched + v6_obs_matched + v7_obs_matched + v8_obs_matched + v9_obs_matched + v10_obs_matched + (1+trial|ID))
#Group is a function of each of the variables

```

```{r}
#Setting up priors

get_prior(data=train_inf, 
          family = bernoulli, 
          f1)

p1 <- c(
  prior(normal(0,1), class=Intercept),
  prior(normal(0,0.3), class=b), 
  prior(normal(0,0.3), class= sd)
)


```

```{r}
#Fitting the model to priors only

m1_prior <-brm(
  f1,
  data=train_inf,
  family = bernoulli(link = "logit"),
  prior = p1,
  sample_prior = "only",
  backend = "cmdstanr",
  threads = threading(2),
  iter = 1000,
  chains = 2,
  cores = 2,
  control = list(
    adapt_delta = 0.9,
    max_treedepth = 20)
)

m1_prior


# Running Prior predictive check
pp_check(m1_prior, ndraws =1000)
```


### Fitting the model to the data
```{r}
m1 <- brm(
  f1,
  data=train_inf,
  family = bernoulli,
  prior = p1,
  sample_prior = T,
  backend = "cmdstanr",
  threads = threading(2),
  iter = 1000,
  chains = 2,
  cores = 2,
  control = list(
    adapt_delta = 0.9,
    max_treedepth = 20)
)

pp_check(m1, ndraws = 1000)
m1
#plot(m1)
```


```{r}
#Prior predictive update checks for the informed data
m1_posterior <- as_draws_df(m1) 

#The prior-posterior update plot for the intercept beta:
plot1_I <- ggplot(m1_posterior) +
geom_histogram(aes(prior_Intercept), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_Intercept), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Intercept') +
labs(title="Intercept: Prior-posterior update check of the effect size") +
theme_classic()

plot1 <- ggplot(m1_posterior) +
geom_histogram(aes(prior_b), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_Intercept), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v1: Prior-posterior update check of the effect size") +
theme_classic()

plot2 <- ggplot(m1_posterior) +
geom_histogram(aes(prior_b), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v2_obs_matched), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v2: Prior-posterior update check of the effect size") +
theme_classic()

plot3 <- ggplot(m1_posterior) +
geom_histogram(aes(prior_b), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v3_obs_matched), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v3: Prior-posterior update check of the effect size") +
theme_classic()

plot4 <- ggplot(m1_posterior) +
geom_histogram(aes(prior_b), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v4_obs_matched), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v4: Prior-posterior update check of the effect size") +
theme_classic()

plot5 <- ggplot(m1_posterior) +
geom_histogram(aes(prior_b), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v5_obs_matched), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v5: Prior-posterior update check of the effect size") +
theme_classic()

plot6 <- ggplot(m1_posterior) +
geom_histogram(aes(prior_b), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v6_obs_matched), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v6: Prior-posterior update check of the effect size") +
theme_classic()

plot7 <- ggplot(m1_posterior) +
geom_histogram(aes(prior_b), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v7_obs_matched), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v7: Prior-posterior update check of the effect size") +
theme_classic()

plot8 <- ggplot(m1_posterior) +
geom_histogram(aes(prior_b), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v8_obs_matched), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v_8 Prior-posterior update check of the effect size") +
theme_classic()

plot9 <- ggplot(m1_posterior) +
geom_histogram(aes(prior_b), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v9_obs_matched), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v9: Prior-posterior update check of the effect size") +
theme_classic()

plot10 <- ggplot(m1_posterior) +
geom_histogram(aes(prior_b), fill="darkblue", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v10_obs_matched), fill="darkgreen", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v10: Prior-posterior update check of the effect size") +
theme_classic()

```


```{r}
grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6)
grid.arrange(plot7, plot8, plot9, plot10, plot1_I)

```


## Doing the same thing for the *noise* training data
```{r}
m1_noise <- update(m1, newdata = train_noise)

pp_check(m1_noise, ndraws=1000)
m1_noise

```

```{r}
m1_posterior_noise <- as_draws_df(m1_noise) 

#The prior-posterior update plot for the betas:
plot1_noise <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_b), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v1_obs_matched), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v1: Prior-posterior update check of the noise effect size") +
theme_classic()

plot2_noise <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_b), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v2_obs_matched), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v2: Prior-posterior update check of the noise effect size") +
theme_classic()

plot3_noise <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_b), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v3_obs_matched), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v3: Prior-posterior update check of the noise effect size") +
theme_classic()

plot4_noise <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_b), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v4_obs_matched), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v4: Prior-posterior update check of the noise effect size") +
theme_classic()

plot5_noise <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_b), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v5_obs_matched), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v5: Prior-posterior update check of the noise effect size") +
theme_classic()

plot6_noise <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_b), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v6_obs_matched), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v6: Prior-posterior update check of the noise effect size") +
theme_classic()

plot7_noise <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_b), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v7_obs_matched), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v7: Prior-posterior update check of the noise effect size") +
theme_classic()

plot8_noise <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_b), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v8_obs_matched), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v_8 Prior-posterior update check of the noise effect size") +
theme_classic()

plot9_noise <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_b), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v9_obs_matched), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Beta') +
labs(title="v9: Prior-posterior update check of the noise effect size") +
theme_classic()

plot10_noise <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_b), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_v10_obs_matched), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Beta') + 
labs(title="v10: Prior-posterior update check of the noise effect size") +
theme_classic()

#The prior-posterior update plot for the intercept:
plot1_noise_I <- ggplot(m1_posterior_noise) +
geom_histogram(aes(prior_Intercept), fill="purple", color="black",alpha=0.6, bins = 30) +
geom_histogram(aes(b_Intercept), fill="green", color="black",alpha=0.6, bins = 30)+
xlab('Intercept') +
labs(title="v1: Prior-posterior update check on the intercept of the noise effect size") +
theme_classic()

```


```{r}
grid.arrange(plot1_noise, plot2_noise, plot3_noise, plot4_noise, plot5_noise, plot6_noise)
grid.arrange(plot7_noise, plot8_noise, plot9_noise, plot10_noise, plot1_noise_I)
```

#Plottig conditional effects
```{r}
brms::conditional_effects(m1)
brms::conditional_effects(m1_noise)
```




# Assessing performance on the informed data
```{r}
# #train_inf
# train_inf$pred_perc <- predict(m1)[,1] #one prediction per participant
# train_inf <- train_inf %>% mutate(pred =as.factor(ifelse(pred_perc > 0.5, "HC", "SZ"))) #deciding if the prediction is HC/SZ
# 
# train_inf$pred <- factor(train_inf$pred, levels=c('SZ', 'HC'))
# 
# 
# #train_noise
# train_noise$pred_perc <- predict(m1_noise)[,1] #one prediction per participant
# train_noise <- train_noise %>% mutate(pred =as.factor(ifelse(pred_perc > 0.5, "HC", "SZ")))#deciding if the prediction is HC/SZ
# 
# train_noise$pred <- factor(train_noise$pred, levels=c('SZ', 'HC'))

```

```{r}
# #Train_inf performance
# yardstick::conf_mat(train_inf, 
#          truth = group,
#          estimate = pred,
#          dnn = c("Prediction", "Truth")) #Confusion matrix of how many participants were correctly guessed
# 
# metrics(train_inf, 
#         truth = group, 
#         estimate = pred) #.estimate is an average estimate of how well the model predicts
# 
# 
# #Train_noise performance
# yardstick::conf_mat(train_noise, 
#          truth = group,
#          estimate = av_pred,
#          dnn = c("Prediction", "Truth")) #Confusion matrix of how many participants were correctly guessed
# 
# metrics(train_noise, 
#          truth = group, 
#         estimate = av_pred)#.estimate is an average estimate of how well the model predicts
# 


```





# Predictions on the test data - both informed and noise

```{r}

#inf

test_inf_s$av_pred_perc <- predict(m1,newdata = test_inf_s, allow_new_levels=T)[,1]
test_inf_s <- test_inf_s %>% mutate(av_pred =as.factor(ifelse(av_pred_perc > 0.5, "HC", "SZ")))

test_inf_s$av_pred <- factor(test_inf_s$av_pred, levels=c('SZ', 'HC'))



#noise

test_noise_s$av_pred_perc <- predict(m1_noise, newdata = test_noise_s, allow_new_levels=T)[,1]
test_noise_s <- test_noise_s %>% mutate(av_pred =as.factor(ifelse(av_pred_perc > 0.5, "HC", "SZ")))

test_noise_s$av_pred <- factor(test_noise_s$av_pred, levels=c('SZ', 'HC'))

# #inf
# 
# test_inf_s$av_pred_perc <- predict(m1,newdata = test_inf_s, allow_new_levels=T)[,1]
# test_inf_s <- test_inf_s %>% mutate(av_pred =as.factor(ifelse(av_pred_perc > 0.5, "HC", "SZ")))
# 
# test_inf_s$av_pred <- factor(test_inf_s$av_pred, levels=c('SZ', 'HC'))
# 
# 
# 
# #noise
# 
# test_noise_s$av_pred_perc <- predict(m1, newdata = test_noise_s, allow_new_levels=T)[,1]
# test_noise_s <- test_noise_s %>% mutate(av_pred =as.factor(ifelse(av_pred_perc > 0.5, "HC", "SZ")))
# 
# test_noise_s$av_pred <- factor(test_noise_s$av_pred, levels=c('SZ', 'HC'))
# 
# 
# 
# 
# #test_inf
# test_inf_s$pred_perc <- predict(m1)[,1] #one prediction per participant
# train_inf <- train_inf %>% mutate(pred =as.factor(ifelse(pred_perc > 0.5, "HC", "SZ"))) #deciding if the prediction is HC/SZ
# 
# train_inf$pred <- factor(train_inf$pred, levels=c('SZ', 'HC'))
# 
# 
# #test_noise
# train_noise$pred_perc <- predict(m1_noise)[,1] #one prediction per participant
# train_noise <- train_noise %>% mutate(pred =as.factor(ifelse(pred_perc > 0.5, "HC", "SZ")))#deciding if the prediction is HC/SZ
# 
# train_noise$pred <- factor(train_noise$pred, levels=c('SZ', 'HC'))

```


```{r}
# #Test_inf performance
# yardstick::conf_mat(test_inf_s, 
#          truth = group,
#          estimate = av_pred,
#          dnn = c("Prediction", "Truth"))
# 
# 
# #Test_noise performance
# yardstick::conf_mat(test_noise_s, 
#          truth = group,
#          estimate = av_pred,
#          dnn = c("Prediction", "Truth"))
# 
# metrics(test_inf_s, 
#          truth = group, 
#         estimate = av_pred)

```



#Predictions from every posterior sample
```{r}
perf_prob <-  tibble(expand.grid(
  sample = seq(1000),
  data = c("informed", "noise"),
  type = c("training", "test"))
)

#informed
train <- inv_logit_scaled(posterior_linpred(m1, summary = F))
test <- inv_logit_scaled(posterior_linpred(m1, newdata = test_inf_s, summary = F, allow_new_levels = T))

#noise
train_n <- inv_logit_scaled(posterior_linpred(m1_noise, summary = F))
test_n <- inv_logit_scaled(posterior_linpred(m1_noise, newdata = test_noise_s, summary = F, allow_new_levels = T))

fun_if <- function(x) {ifelse(x > 0.5, "HC", "SZ")}

```

```{r}
for (i in seq(1000)){
  train_inf$predictions <- as.factor(ifelse(train[i,] > 0.5, "HC", "SZ")) %>%  factor(levels=c('SZ', 'HC'))
  test_inf_s$predictions <- as.factor(ifelse(test[i,] > 0.5, "HC", "SZ")) %>% factor(levels=c('SZ', 'HC'))
  
    train_noise$predictions <- as.factor(ifelse(train_n[i,] > 0.5, "HC", "SZ")) %>%  factor(levels=c('SZ', 'HC'))
  test_noise_s$predictions <- as.factor(ifelse(test_n[i,] > 0.5, "HC", "SZ")) %>% factor(levels=c('SZ', 'HC'))
  
  
  
  perf_prob$accuracy[perf_prob$sample==i & perf_prob$data == "informed" & perf_prob$type=="training"] <- accuracy(train_inf, truth = group, estimate = predictions)[, ".estimate"]
  
   perf_prob$accuracy[perf_prob$sample==i & perf_prob$data == "informed" & perf_prob$type=="test"] <- yardstick::accuracy(test_inf_s, truth = group, estimate = predictions)[, ".estimate"]
   
    perf_prob$accuracy[perf_prob$sample==i & perf_prob$data == "noise" & perf_prob$type=="training"] <- accuracy(train_noise, truth = group, estimate = predictions)[, ".estimate"]
    
     perf_prob$accuracy[perf_prob$sample==i & perf_prob$data == "noise" & perf_prob$type=="test"] <- yardstick::accuracy(test_noise_s, truth = group, estimate = predictions)[, ".estimate"]



train_inf_pred <- as_tibble(train)
test_inf_s_pred <- as_tibble(test)
train_noise_pred <- as_tibble(train_n)
test_noise_s_pred <- as_tibble(test_n)
 
train_inf_pred <- train_inf_pred %>% apply(2, fun_if) %>% as_tibble()
test_inf_s_pred <- test_inf_s_pred %>% apply(2, fun_if) %>% as_tibble()
train_noise_pred <- train_noise_pred %>% apply(2, fun_if) %>% as_tibble()
test_noise_s_pred <- test_noise_s_pred %>% apply(2, fun_if) %>% as_tibble()

train_inf_pred[sapply(train_inf_pred, is.character)] <- lapply(train_inf_pred[sapply(train_inf_pred, is.character)], as.factor)
test_inf_s_pred[sapply(test_inf_s_pred, is.character)] <- lapply(test_inf_s_pred[sapply(test_inf_s_pred, is.character)], as.factor)
train_noise_pred[sapply(train_noise_pred, is.character)] <- lapply(train_noise_pred[sapply(train_noise_pred, is.character)], as.factor)
test_noise_s_pred[sapply(test_noise_s_pred, is.character)] <- lapply(test_noise_s_pred[sapply(test_noise_s_pred, is.character)], as.factor)

for (i in seq(1000)) {
  train_inf$pred <- c(train_inf_pred[i,])
  train_inf <- unnest(train_inf, pred)
  train_inf$pred <- factor(train_inf$pred, levels=c('SZ', 'HC'))
  test_inf_s$pred <- c(test_inf_s_pred[i,])
  test_inf_s <- unnest(test_inf_s, pred)
  test_inf_s$pred <- factor(test_inf_s$pred, levels=c('SZ', 'HC'))
  train_noise$pred <- c(train_noise_pred[i,])
  train_noise <- unnest(train_noise, pred)
  train_noise$pred <- factor(train_noise$pred, levels=c('SZ', 'HC'))
  test_noise_s$pred <- c(test_noise_s_pred[i,])
  test_noise_s <- unnest(test_noise_s, pred)
  test_noise_s$pred <- factor(test_noise_s$pred, levels=c('SZ', 'HC'))

  perf_prob$accuracy[perf_prob$sample==i & perf_prob$data == "informed" & perf_prob$type=="training"] <- accuracy(train_inf, truth = group, estimate = pred)[, ".estimate"]
  perf_prob$accuracy[perf_prob$sample==i & perf_prob$data == "informed" & perf_prob$type=="test"] <- (accuracy(test_inf_s, truth = group, estimate = pred)[, ".estimate"])
  perf_prob$accuracy[perf_prob$sample==i & perf_prob$data == "noise" & perf_prob$type=="training"] <- accuracy(train_noise, truth = group, estimate = pred)[, ".estimate"]
  perf_prob$accuracy[perf_prob$sample==i & perf_prob$data == "noise" & perf_prob$type=="test"] <- (accuracy(test_noise_s, truth = group, estimate = pred)[, ".estimate"])

}

```

# Plotting the predictions w. uncertainty
```{r}
perf_prob <- unnest(perf_prob, accuracy)

ggplot(perf_prob, aes(y=as.numeric(accuracy) , x=type, color=type)) +
  geom_point(data = perf_prob)+
  scale_y_continuous() + 
  facet_wrap(~ data)

  
```



# Feature importance


```{r}
train_trees <- train_inf %>% mutate(ID = NULL,
                                    trial = NULL, 
                                    av_pred = NULL, 
                                    av_pred_perc = NULL, 
                                    v3_obs_matched = NULL, 
                                    predictions = NULL)



#SVM-engine
informed_SVM <- svm_rbf()%>% 
  set_mode("classification") %>% 
  set_engine("kernlab") %>% 
  fit(group ~., data = train_trees)


am_i_important <- explain_tidymodels(informed_SVM, 
                                     data = train_inf, 
                                     y= as.numeric(train_inf$group)-1,
                                     label= "SVM", 
                                     verbose = FALSE)



#RandomForest-engine
informed_trees <- rand_forest()%>% 
  set_mode("classification") %>% 
  set_engine("randomForest") %>% 
  fit(group ~., data = train_trees)


am_i_important_2 <- explain_tidymodels(informed_trees, 
                                     data = train_inf, 
                                     y= as.numeric(train_inf$group)-1,
                                     label= "Random Forest", 
                                     verbose = FALSE)


am_i_important %>% model_parts() %>% plot() #SVM
am_i_important_2 %>% model_parts() %>% plot() #RandomForest


```

#Checking wheter the noise variables also show up as important predictors on the test data
```{r}
test_trees <- test_inf_s %>% mutate(ID = NULL, 
                                    trial = NULL,
                                    av_pred = NULL, 
                                    av_pred_perc = NULL,
                                    v3_obs_matched = NULL, 
                                    predictions = NULL)


#SVM-engine
informed_SVM_test <- svm_rbf()%>% 
  set_mode("classification") %>% 
  set_engine("kernlab") %>% 
  fit(group ~., data = test_trees)


am_i_really_important <- explain_tidymodels(informed_SVM, 
                                     data = test_inf_s, 
                                     y= as.numeric(test_inf_s$group)-1,
                                     label= "SVM", 
                                     verbose = FALSE)



#RandomForest-engine
informed_trees_test <- rand_forest()%>% 
  set_mode("classification") %>% 
  set_engine("randomForest") %>% 
  fit(group ~., data = test_trees)


am_i_really_important_2 <- explain_tidymodels(informed_trees, 
                                     data = test_inf_s, 
                                     y= as.numeric(test_inf_s$group)-1,
                                     label= "Random Forest", 
                                     verbose = FALSE)


am_i_really_important %>% model_parts() %>% plot() #SVM
am_i_really_important_2 %>% model_parts() %>% plot() #RandomForest

```




Part III

Download the empirical dataset from brightspace and apply your ML pipeline to the new data, adjusting where needed. *Warning*: in the simulated dataset we only had 10 features, now you have many more! Such is the life of the ML practitioner. Consider the impact a higher number of features will have on your ML inference, and decide whether you need to cut down the number of features before running the pipeline (or alternatively expand the pipeline to add feature selection).

Data: https://www.dropbox.com/s/7ky1axvea33lgye/Ass3_empiricalData1.csv?dl=0

```{r}
real_data <- read_csv("Ass3_empiricalData1.csv")
```


```{r}
#removing non-acoustic features
real_data <- real_data %>%
  mutate(PatID = NULL,
    Gender= as.factor(Gender), 
    NewID = as.factor(NewID),
    Trial = gsub("T", "", as.factor(Trial)),
     Trial = as.factor(Trial),
    Language = NULL,
    Corpus = NULL,
    Diagnosis = as.factor(Diagnosis)
  )
```


#Data budgetting
```{r}
partition <- partition(
  real_data, 
  p = 0.8, 
  cat_col = "Diagnosis",
   id_col = "NewID", 
  list_out = F
  )


train_part <- partition %>% filter(.partitions == 1) 
test_part <- partition %>% filter(.partitions == 2) 

```


# Standardizing test and training set
```{r}
part_rec <- train_part %>% recipe(Diagnosis ~.) %>% 
  update_role(NewID, Trial, Gender,  new_role = "Second level") %>%
  update_role_requirements("Second level", bake=F) %>% 
  step_scale(all_numeric()) %>%
  step_center(all_numeric()) %>%
  prep(train=train_part, retain = T)
  

#Using the recipe on the test data
train_real <- juice(part_rec)
test_real <- bake(part_rec, new_data = test_part)


```


#Variable selection by correlation
```{r}
train_num <- select_if(train_real, is.numeric)
train_factor <- select_if(train_real, is.factor)

#Finding the highly correlated variables. Setting the threshold to 0.65
cutoffs <- findCorrelation(cor(train_num), cutoff = 0.65)

new_df <- train_num[,-cutoffs]
t <- cbind(new_df, train_factor)

plot_correlation(t)

```


#PCA - feature selcetion inception
```{r}

pca_train_ <-  recipe(Diagnosis ~., data = t) %>% 
  update_role(NewID, Trial, Gender,  new_role = "id") %>%
  step_scale(all_numeric()) %>%
  step_center(all_numeric()) %>%
  step_pca(all_numeric(), threshold = 0.5)

pca_train_2 <-recipes::prep(pca_train_, retain = T)

pca_train_2 <- tidy(pca_train_2, 3)



pca_train_2 %>%
  filter(component %in% paste0("PC", 1:5)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~component, nrow = 1) +
  labs(y = NULL)


pca1 <- pca_train_2 %>% filter(component == "PC1") %>% filter(value < 0.05) 

terms <- pca1$terms
t_new <- t[,!(names(t) %in% terms)] 

t_new$.partitions <- NULL

```


```{r}
n <- names(t_new)
n <- n[-c(31,33)]
n

```
